{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 11: Natural Language Processing and Speech Recognition**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reused Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart, we will see more of this chart in the next class.\n",
    "def chart_regression(pred, y):\n",
    "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
    "    t.sort_values(by=['y'], inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(), label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(), label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a new directory to hold checkpoints from a neural network.  This allows the neural network to be\n",
    "# loaded later.  If the erase param is set to true, the contents of the directory will be cleared.\n",
    "def get_model_dir(name, erase):\n",
    "    base_path = os.path.join(\".\", \"dnn\")\n",
    "    model_dir = os.path.join(base_path, name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    if erase and len(model_dir) > 4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir, ignore_errors=True)  # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with LSTM and CNN Neural Networks\n",
    "\n",
    "The material in this class session is based heavily upon the paper [Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626).\n",
    "\n",
    "The following pages were also used for material for this class session:\n",
    "\n",
    "* [TensorFlow — Text Classification](https://medium.com/@ilblackdragon/tensorflow-text-classification-615198df9231#.i1r4ao3te)\n",
    "\n",
    "TensorFlow implementations of the above paper:\n",
    "\n",
    "* [Text Classification Using Recurrent Neural Networks on Words]()\n",
    "* [Text Classification Using Convolutional Neural Networks on Words]()\n",
    "* [Text Classification Using Recurrent Neural Networks on Characters]()\n",
    "* [Text Classification Using Convolutional Neural Networks on Characters]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources: DBPedia\n",
    "\n",
    "[DBPedia](http://wiki.dbpedia.org/) uses the data contained in [WikiPedia]() in database form.  The data in DBPedia can be queried in an SQL-like syntax named Protocol and RDF Query Language, or [SPARQL](https://en.wikipedia.org/wiki/SPARQL). \n",
    "\n",
    "For the text examples in this class we will use a sample of the DBPedia articles classified into 14 high level document classifications:\n",
    "\n",
    "* Company (1)\n",
    "* EducationalInstitution (2)\n",
    "* Artist (3)\n",
    "* Athlete (4)\n",
    "* OfficeHolder (5)\n",
    "* MeanOfTransportation (6)\n",
    "* Building (7)\n",
    "* NaturalPlace (8)\n",
    "* Village (9)\n",
    "* Animal (10)\n",
    "* Plant (11)\n",
    "* Album (12)\n",
    "* Film (13)\n",
    "* WrittenWork (14)\n",
    "\n",
    "The data files can be found at this [location](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow makes available several operators designed for text classification.\n",
    "\n",
    "* skflow.preprocessing.**ByteProcessor (doc_len)** - Turn a list of text strings into fixed length arrays (specified by doc_len) using integer ASCII values, for example \"ABC\" becomes [65, 66, 67, 0, 0] if the doc_len is 5.\n",
    "* skflow.ops.**one_hot_matrix** - One hot is the same as dummy variables. Expands multiple inputs into a cube, with dimensions [num_samples, input_size, num_samples].\n",
    "* skflow.ops.**split_squeeze** - Splits input on given dimension and then squeezes that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 84, 104, 105, 115,  32], dtype=uint8), array([65, 66, 67,  0,  0], dtype=uint8), array([97, 98, 99,  0,  0], dtype=uint8)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "\n",
    "# Classifying Text Documents\n",
    "\n",
    "data = [\n",
    "    \"This is a test\",\n",
    "    \"ABC\",\n",
    "    \"abc\"\n",
    "]\n",
    "\n",
    "char_processor = learn.preprocessing.ByteProcessor(5)\n",
    "\n",
    "z = list(char_processor.fit_transform(data))\n",
    "\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:Tensor(\"OneHot:0\", shape=(560000, 100, 256), dtype=float32)\n",
      "100\n",
      "2:Tensor(\"Squeeze:0\", shape=(560000, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp = skflow.ops.one_hot_matrix(X_train, 256) \n",
    "print(\"1:{}\".format(temp))\n",
    "temp = skflow.ops.split_squeeze(1, MAX_DOCUMENT_LENGTH, temp)\n",
    "print(len(temp))\n",
    "print(\"2:{}\".format(temp[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "\n",
    "Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781). arXiv preprint arXiv:1301.3781.\n",
    "\n",
    "![Word2Vec](https://pbs.twimg.com/media/C7jJxIjWkAA8E_s.jpg)\n",
    "[Trust Word2Vec](https://twitter.com/DanilBaibak/status/844647217885581312)\n",
    "\n",
    "### Suggested Software for Word2Vec\n",
    "\n",
    "* [GoogleNews Vectors](https://code.google.com/archive/p/word2vec/), [GitHub Mirror](https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n",
    "* [Python Gensim](https://radimrehurek.com/gensim/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Not that the path below refers to a location on my hard drive.\n",
    "# You should download GoogleNews Vectors (see suggested software above)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/Users/jeff/data/language/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = model['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05419922  0.01708984 -0.00527954  0.33203125 -0.25       -0.01397705\n",
      " -0.15039062 -0.265625    0.01647949  0.3828125  -0.03295898 -0.09716797\n",
      " -0.16308594 -0.04443359  0.00946045  0.18457031  0.03637695  0.16601562\n",
      "  0.36328125 -0.25585938  0.375       0.171875    0.21386719 -0.19921875\n",
      "  0.13085938 -0.07275391 -0.02819824  0.11621094  0.15332031  0.09082031\n",
      "  0.06787109 -0.0300293  -0.16894531 -0.20800781 -0.03710938 -0.22753906\n",
      "  0.26367188  0.012146    0.18359375  0.31054688 -0.10791016 -0.19140625\n",
      "  0.21582031  0.13183594 -0.03515625  0.18554688 -0.30859375  0.04785156\n",
      " -0.10986328  0.14355469 -0.43554688 -0.0378418   0.10839844  0.140625\n",
      " -0.10595703  0.26171875 -0.17089844  0.39453125  0.12597656 -0.27734375\n",
      " -0.28125     0.14746094 -0.20996094  0.02355957  0.18457031  0.00445557\n",
      " -0.27929688 -0.03637695 -0.29296875  0.19628906  0.20703125  0.2890625\n",
      " -0.20507812  0.06787109 -0.43164062 -0.10986328 -0.2578125  -0.02331543\n",
      "  0.11328125  0.23144531 -0.04418945  0.10839844 -0.2890625  -0.09521484\n",
      " -0.10351562 -0.0324707   0.07763672 -0.13378906  0.22949219  0.06298828\n",
      "  0.08349609  0.02929688 -0.11474609  0.00534058 -0.12988281  0.02514648\n",
      "  0.08789062  0.24511719 -0.11474609 -0.296875   -0.59375    -0.29492188\n",
      " -0.13378906  0.27734375 -0.04174805  0.11621094  0.28320312  0.00241089\n",
      "  0.13867188 -0.00683594 -0.30078125  0.16210938  0.01171875 -0.13867188\n",
      "  0.48828125  0.02880859  0.02416992  0.04736328  0.05859375 -0.23828125\n",
      "  0.02758789  0.05981445 -0.03857422  0.06933594  0.14941406 -0.10888672\n",
      " -0.07324219  0.08789062  0.27148438  0.06591797 -0.37890625 -0.26171875\n",
      " -0.13183594  0.09570312 -0.3125      0.10205078  0.03063965  0.23632812\n",
      "  0.00582886  0.27734375  0.20507812 -0.17871094 -0.31445312 -0.01586914\n",
      "  0.13964844  0.13574219  0.0390625  -0.29296875  0.234375   -0.33984375\n",
      " -0.11816406  0.10644531 -0.18457031 -0.02099609  0.02563477  0.25390625\n",
      "  0.07275391  0.13574219 -0.00138092 -0.2578125  -0.2890625   0.10107422\n",
      "  0.19238281 -0.04882812  0.27929688 -0.3359375  -0.07373047  0.01879883\n",
      " -0.10986328 -0.04614258  0.15722656  0.06689453 -0.03417969  0.16308594\n",
      "  0.08642578  0.44726562  0.02026367 -0.01977539  0.07958984  0.17773438\n",
      " -0.04370117 -0.00952148  0.16503906  0.17285156  0.23144531 -0.04272461\n",
      "  0.02355957  0.18359375 -0.41601562 -0.01745605  0.16796875  0.04736328\n",
      "  0.14257812  0.08496094  0.33984375  0.1484375  -0.34375    -0.14160156\n",
      " -0.06835938 -0.14648438 -0.02844238  0.07421875 -0.07666016  0.12695312\n",
      "  0.05859375 -0.07568359 -0.03344727  0.23632812 -0.16308594  0.16503906\n",
      "  0.1484375  -0.2421875  -0.3515625  -0.30664062  0.00491333  0.17675781\n",
      "  0.46289062  0.14257812 -0.25       -0.25976562  0.04370117  0.34960938\n",
      "  0.05957031  0.07617188 -0.02868652 -0.09667969 -0.01281738  0.05859375\n",
      " -0.22949219 -0.1953125  -0.12207031  0.20117188 -0.42382812  0.06005859\n",
      "  0.50390625  0.20898438  0.11230469 -0.06054688  0.33203125  0.07421875\n",
      " -0.05786133  0.11083984 -0.06494141  0.05639648  0.01757812  0.08398438\n",
      "  0.13769531  0.2578125   0.16796875 -0.16894531  0.01794434  0.16015625\n",
      "  0.26171875  0.31640625 -0.24804688  0.05371094 -0.0859375   0.17089844\n",
      " -0.39453125 -0.00156403 -0.07324219 -0.04614258 -0.16210938 -0.15722656\n",
      "  0.21289062 -0.15820312  0.04394531  0.28515625  0.01196289 -0.26953125\n",
      " -0.04370117  0.37109375  0.04663086 -0.19726562  0.3046875  -0.36523438\n",
      " -0.23632812  0.08056641 -0.04248047 -0.14648438 -0.06225586 -0.0534668\n",
      " -0.05664062  0.18945312  0.37109375 -0.22070312  0.04638672  0.02612305\n",
      " -0.11474609  0.265625   -0.02453613  0.11083984 -0.02514648 -0.12060547\n",
      "  0.05297852  0.07128906  0.00063705 -0.36523438 -0.13769531 -0.12890625]\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.08153\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = model['cat']\n",
    "w2 = model['dog']\n",
    "\n",
    "dist = np.linalg.norm  (w1-w2)\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236843824386597),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134939193726),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76640122309953529"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# **Code below this point will not run in Data Scientist Workbench **\n",
    "\n",
    "The code below interfaces with your computer's microphone and speakers.  It will not run in Data Scientist Workbench.\n",
    "\n",
    "\n",
    "# Speech Recognition\n",
    "\n",
    "A very common use of LSTM and RNN's is [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Voice for Speech Recognition\n",
    "\n",
    "Google speech recognition makes use of [LSTM and some other technologies](https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html).\n",
    "\n",
    "See Google [Speech Recognition in action](https://www.google.com/intl/en/chrome/demos/speech.html).\n",
    "\n",
    "```\n",
    "pip install SpeechRecognition\n",
    "```\n",
    "For Mac install, see the [following](http://stackoverflow.com/questions/33513522/when-installing-pyaudio-pip-cannot-find-portaudio-h-in-usr-local-include)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "You said: hello\n"
     ]
    }
   ],
   "source": [
    "# pip install SpeechRecognition\n",
    "# see this for PyAudio\n",
    "# pip install pyttsx\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# NOTE: this example requires PyAudio because it uses the Microphone class\n",
    "\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "# obtain audio from the microphone\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# recognize speech using Google Speech Recognition\n",
    "try:\n",
    "    # for testing purposes, we're just using the default API key\n",
    "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "    # instead of `r.recognize_google(audio)`\n",
    "    str = r.recognize_google(audio)\n",
    "    print(\"You said: {}\".format(str))\n",
    "    os.system(\"say 'I believe you said: {}'\".format(str))\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text to Speech\n",
    "\n",
    "Challenges:\n",
    "\n",
    "* [Background Conversation](https://www.youtube.com/watch?v=IKB3Qiglyro&t=119s)\n",
    "* [Klingon](https://www.youtube.com/watch?v=ucO3heC-Ztw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code works on a Mac\n",
    "import os\n",
    "\n",
    "def say(s):\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    os.system(\"say '{}'\".format(s))\n",
    "    \n",
    "say(\"Shall we play a game?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Speech and Speech Recognition\n",
    "\n",
    "\n",
    "Text to speech and speech recognition often go hand in hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: hola que tal como se Yama\n"
     ]
    }
   ],
   "source": [
    "# pip install SpeechRecognition\n",
    "# see this for PyAudio\n",
    "# pip install pyttsx\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# NOTE: this example requires PyAudio because it uses the Microphone class\n",
    "\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "def say(s):\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    os.system(\"say '{}'\".format(s))\n",
    "\n",
    "# obtain audio from the microphone\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    say(\"Hello there, please say something.\")\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# recognize speech using Google Speech Recognition\n",
    "try:\n",
    "    # for testing purposes, we're just using the default API key\n",
    "    # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "    # instead of `r.recognize_google(audio)`\n",
    "    str = r.recognize_google(audio)\n",
    "    print(\"You said: {}\".format(str))\n",
    "    say(\"I think you said {}\".format(str))\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliza Example\n",
    "\n",
    "[ELIZA](https://en.wikipedia.org/wiki/ELIZA) is an early natural language processing computer program created from 1964 to 1966 at the MIT Artificial Intelligence Laboratory by Joseph Weizenbaum.  The following code is based in an [Eliza Python Implementation by SureSmallThing](https://www.smallsurething.com/implementing-the-famous-eliza-chatbot-in-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: my mother hates me\n",
      "Eliza (computer): When your mother hates you, how do you feel?\n",
      "Human: I feel sad\n",
      "Eliza (computer): you feel sad.\n",
      "Human: yes I do do you feel sad\n",
      "Eliza (computer): OK, but can you elaborate a bit?\n",
      "No input, or could not understand audio.\n",
      "Human: quick\n",
      "Eliza (computer): Why do you say that quick?\n",
      "Human: quick\n",
      "Eliza (computer): quick.\n",
      "Human: quit\n",
      "Eliza (computer): Thank you for talking with me.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "reflections = {\n",
    "    \"am\": \"are\",\n",
    "    \"was\": \"were\",\n",
    "    \"i\": \"you\",\n",
    "    \"i'd\": \"you would\",\n",
    "    \"i've\": \"you have\",\n",
    "    \"i'll\": \"you will\",\n",
    "    \"my\": \"your\",\n",
    "    \"are\": \"am\",\n",
    "    \"you've\": \"I have\",\n",
    "    \"you'll\": \"I will\",\n",
    "    \"your\": \"my\",\n",
    "    \"yours\": \"mine\",\n",
    "    \"you\": \"me\",\n",
    "    \"me\": \"you\"\n",
    "}\n",
    "\n",
    "psychobabble = [\n",
    "    [r'i need (.*)',\n",
    "     [\"Why do you need {0}?\",\n",
    "      \"Would it really help you to get {0}?\",\n",
    "      \"Are you sure you need {0}?\"]],\n",
    "\n",
    "    [r'why don\\'?t you ([^\\?]*)\\??',\n",
    "     [\"Do you really think I don't {0}?\",\n",
    "      \"Perhaps eventually I will {0}.\",\n",
    "      \"Do you really want me to {0}?\"]],\n",
    "\n",
    "    [r'why can\\'?t I ([^\\?]*)\\??',\n",
    "     [\"Do you think you should be able to {0}?\",\n",
    "      \"If you could {0}, what would you do?\",\n",
    "      \"I don't know -- why can't you {0}?\",\n",
    "      \"Have you really tried?\"]],\n",
    "\n",
    "    [r'i can\\'?t (.*)',\n",
    "     [\"How do you know you can't {0}?\",\n",
    "      \"Perhaps you could {0} if you tried.\",\n",
    "      \"What would it take for you to {0}?\"]],\n",
    "\n",
    "    [r'i am (.*)',\n",
    "     [\"Did you come to me because you are {0}?\",\n",
    "      \"How long have you been {0}?\",\n",
    "      \"How do you feel about being {0}?\"]],\n",
    "\n",
    "    [r'i\\'?m (.*)',\n",
    "     [\"How does being {0} make you feel?\",\n",
    "      \"Do you enjoy being {0}?\",\n",
    "      \"Why do you tell me you're {0}?\",\n",
    "      \"Why do you think you're {0}?\"]],\n",
    "\n",
    "    [r'are you ([^\\?]*)\\??',\n",
    "     [\"Why does it matter whether I am {0}?\",\n",
    "      \"Would you prefer it if I were not {0}?\",\n",
    "      \"Perhaps you believe I am {0}.\",\n",
    "      \"I may be {0} -- what do you think?\"]],\n",
    "\n",
    "    [r'what (.*)',\n",
    "     [\"Why do you ask?\",\n",
    "      \"How would an answer to that help you?\",\n",
    "      \"What do you think?\"]],\n",
    "\n",
    "    [r'how (.*)',\n",
    "     [\"How do you suppose?\",\n",
    "      \"Perhaps you can answer your own question.\",\n",
    "      \"What is it you're really asking?\"]],\n",
    "\n",
    "    [r'because (.*)',\n",
    "     [\"Is that the real reason?\",\n",
    "      \"What other reasons come to mind?\",\n",
    "      \"Does that reason apply to anything else?\",\n",
    "      \"If {0}, what else must be true?\"]],\n",
    "\n",
    "    [r'(.*) sorry (.*)',\n",
    "     [\"There are many times when no apology is needed.\",\n",
    "      \"What feelings do you have when you apologize?\"]],\n",
    "\n",
    "    [r'hello(.*)',\n",
    "     [\"Hello... I'm glad you could drop by today.\",\n",
    "      \"Hi there... how are you today?\",\n",
    "      \"Hello, how are you feeling today?\"]],\n",
    "\n",
    "    [r'i think (.*)',\n",
    "     [\"Do you doubt {0}?\",\n",
    "      \"Do you really think so?\",\n",
    "      \"But you're not sure {0}?\"]],\n",
    "\n",
    "    [r'(.*) friend (.*)',\n",
    "     [\"Tell me more about your friends.\",\n",
    "      \"When you think of a friend, what comes to mind?\",\n",
    "      \"Why don't you tell me about a childhood friend?\"]],\n",
    "\n",
    "    [r'yes',\n",
    "     [\"You seem quite sure.\",\n",
    "      \"OK, but can you elaborate a bit?\"]],\n",
    "\n",
    "    [r'(.*) computer(.*)',\n",
    "     [\"Are you really talking about me?\",\n",
    "      \"Does it seem strange to talk to a computer?\",\n",
    "      \"How do computers make you feel?\",\n",
    "      \"Do you feel threatened by computers?\"]],\n",
    "\n",
    "    [r'is it (.*)',\n",
    "     [\"Do you think it is {0}?\",\n",
    "      \"Perhaps it's {0} -- what do you think?\",\n",
    "      \"If it were {0}, what would you do?\",\n",
    "      \"It could well be that {0}.\"]],\n",
    "\n",
    "    [r'it is (.*)',\n",
    "     [\"You seem very certain.\",\n",
    "      \"If I told you that it probably isn't {0}, what would you feel?\"]],\n",
    "\n",
    "    [r'can you ([^\\?]*)\\??',\n",
    "     [\"What makes you think I can't {0}?\",\n",
    "      \"If I could {0}, then what?\",\n",
    "      \"Why do you ask if I can {0}?\"]],\n",
    "\n",
    "    [r'can I ([^\\?]*)\\??',\n",
    "     [\"Perhaps you don't want to {0}.\",\n",
    "      \"Do you want to be able to {0}?\",\n",
    "      \"If you could {0}, would you?\"]],\n",
    "\n",
    "    [r'you are (.*)',\n",
    "     [\"Why do you think I am {0}?\",\n",
    "      \"Does it please you to think that I'm {0}?\",\n",
    "      \"Perhaps you would like me to be {0}.\",\n",
    "      \"Perhaps you're really talking about yourself?\"]],\n",
    "\n",
    "    [r'you\\'?re (.*)',\n",
    "     [\"Why do you say I am {0}?\",\n",
    "      \"Why do you think I am {0}?\",\n",
    "      \"Are we talking about you, or me?\"]],\n",
    "\n",
    "    [r'i don\\'?t (.*)',\n",
    "     [\"Don't you really {0}?\",\n",
    "      \"Why don't you {0}?\",\n",
    "      \"Do you want to {0}?\"]],\n",
    "\n",
    "    [r'i feel (.*)',\n",
    "     [\"Good, tell me more about these feelings.\",\n",
    "      \"Do you often feel {0}?\",\n",
    "      \"When do you usually feel {0}?\",\n",
    "      \"When you feel {0}, what do you do?\"]],\n",
    "\n",
    "    [r'i have (.*)',\n",
    "     [\"Why do you tell me that you've {0}?\",\n",
    "      \"Have you really {0}?\",\n",
    "      \"Now that you have {0}, what will you do next?\"]],\n",
    "\n",
    "    [r'i would (.*)',\n",
    "     [\"Could you explain why you would {0}?\",\n",
    "      \"Why would you {0}?\",\n",
    "      \"Who else knows that you would {0}?\"]],\n",
    "\n",
    "    [r'is there (.*)',\n",
    "     [\"Do you think there is {0}?\",\n",
    "      \"It's likely that there is {0}.\",\n",
    "      \"Would you like there to be {0}?\"]],\n",
    "\n",
    "    [r'my (.*)',\n",
    "     [\"I see, your {0}.\",\n",
    "      \"Why do you say that your {0}?\",\n",
    "      \"When your {0}, how do you feel?\"]],\n",
    "\n",
    "    [r'you (.*)',\n",
    "     [\"We should be discussing you, not me.\",\n",
    "      \"Why do you say that about me?\",\n",
    "      \"Why do you care whether I {0}?\"]],\n",
    "\n",
    "    [r'why (.*)',\n",
    "     [\"Why don't you tell me the reason why {0}?\",\n",
    "      \"Why do you think {0}?\"]],\n",
    "\n",
    "    [r'i want (.*)',\n",
    "     [\"What would it mean to you if you got {0}?\",\n",
    "      \"Why do you want {0}?\",\n",
    "      \"What would you do if you got {0}?\",\n",
    "      \"If you got {0}, then what would you do?\"]],\n",
    "\n",
    "    [r'(.*) mother(.*)',\n",
    "     [\"Tell me more about your mother.\",\n",
    "      \"What was your relationship with your mother like?\",\n",
    "      \"How do you feel about your mother?\",\n",
    "      \"How does this relate to your feelings today?\",\n",
    "      \"Good family relations are important.\"]],\n",
    "\n",
    "    [r'(.*) father(.*)',\n",
    "     [\"Tell me more about your father.\",\n",
    "      \"How did your father make you feel?\",\n",
    "      \"How do you feel about your father?\",\n",
    "      \"Does your relationship with your father relate to your feelings today?\",\n",
    "      \"Do you have trouble showing affection with your family?\"]],\n",
    "\n",
    "    [r'(.*) child(.*)',\n",
    "     [\"Did you have close friends as a child?\",\n",
    "      \"What is your favorite childhood memory?\",\n",
    "      \"Do you remember any dreams or nightmares from childhood?\",\n",
    "      \"Did the other children sometimes tease you?\",\n",
    "      \"How do you think your childhood experiences relate to your feelings today?\"]],\n",
    "\n",
    "    [r'(.*)\\?',\n",
    "     [\"Why do you ask that?\",\n",
    "      \"Please consider whether you can answer your own question.\",\n",
    "      \"Perhaps the answer lies within yourself?\",\n",
    "      \"Why don't you tell me?\"]],\n",
    "\n",
    "    [r'quit',\n",
    "     [\"Thank you for talking with me.\",\n",
    "      \"Good-bye.\",\n",
    "      \"Thank you, that will be $150.  Have a good day!\"]],\n",
    "\n",
    "    [r'(.*)',\n",
    "     [\"Please tell me more.\",\n",
    "      \"Let's change focus a bit... Tell me about your family.\",\n",
    "      \"Can you elaborate on that?\",\n",
    "      \"Why do you say that {0}?\",\n",
    "      \"I see.\",\n",
    "      \"Very interesting.\",\n",
    "      \"{0}.\",\n",
    "      \"I see.  And what does that tell you?\",\n",
    "      \"How does that make you feel?\",\n",
    "      \"How do you feel when you say that?\"]]\n",
    "]\n",
    "\n",
    "\n",
    "def reflect(fragment):\n",
    "    tokens = fragment.lower().split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in reflections:\n",
    "            tokens[i] = reflections[token]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def analyze(statement):\n",
    "    for pattern, responses in psychobabble:\n",
    "        match = re.match(pattern, statement.rstrip(\".!\"))\n",
    "        if match:\n",
    "            response = random.choice(responses)\n",
    "            return response.format(*[reflect(g) for g in match.groups()])\n",
    "\n",
    "def say(s):\n",
    "    s = s.replace(\"'\",\"\")\n",
    "    os.system(\"say '{}'\".format(s))\n",
    "\n",
    "def main():\n",
    "    say(\"Hello. How are you feeling today?\")\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            audio = r.listen(source)\n",
    "\n",
    "            # recognize speech using Google Speech Recognition\n",
    "            try:\n",
    "                # for testing purposes, we're just using the default API key\n",
    "                # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "                # instead of `r.recognize_google(audio)`\n",
    "                statement = r.recognize_google(audio)\n",
    "                print(\"Human: {}\".format(statement))\n",
    "                response = analyze(statement)\n",
    "\n",
    "                if statement.lower() == 'quit':\n",
    "                    done = True\n",
    "\n",
    "                print(\"Eliza (computer): {}\".format(response))\n",
    "                say(response)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"No input, or could not understand audio.\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Error: Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bots\n",
    "\n",
    "Using the above code you can create your own primitive chat bots.  A some what famous video on Youtube from Cornell University shows what happens [when two chat bots converse](https://www.youtube.com/watch?v=WnzlbyTZsQY).  Other interesting chat bot type technology:\n",
    "\n",
    "* [CleverBot](http://www.cleverbot.com/)\n",
    "* [Computer Science Paper Generator](https://pdos.csail.mit.edu/archive/scigen/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on LSTM\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [LSTM Music](https://www.youtube.com/watch?v=0VTI1BBLydE)\n",
    "* [Natural Language Processing from Scratch](https://arxiv.org/abs/1103.0398)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for Spring 2017 Kaggle\n",
    "\n",
    "The following code trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_num_ps_replicas': 0, '_save_checkpoints_steps': None, '_task_type': None, '_environment': 'local', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_master': '', '_keep_checkpoint_max': 5, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x116e8bf28>, '_save_summary_steps': 100, '_task_id': 0, '_save_checkpoints_secs': 30}\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From <ipython-input-6-7443ab610f4c>:62: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-6-7443ab610f4c>:62: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./dnn/kaggle/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.69504\n",
      "INFO:tensorflow:global_step/sec: 84.4133\n",
      "INFO:tensorflow:step = 101, loss = 0.683313\n",
      "INFO:tensorflow:global_step/sec: 73.3376\n",
      "INFO:tensorflow:step = 201, loss = 0.636839\n",
      "INFO:tensorflow:global_step/sec: 65.5144\n",
      "INFO:tensorflow:step = 301, loss = 0.614748\n",
      "INFO:tensorflow:global_step/sec: 71.0559\n",
      "INFO:tensorflow:step = 401, loss = 0.581737\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:657: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:657: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-10-01:50:41\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-10-01:50:42\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.712617, auc = 0.832564, global_step = 1, loss = 1.41354\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 500): accuracy = 0.712617, global_step = 1, auc = 0.832564, loss = 1.41354\n",
      "INFO:tensorflow:global_step/sec: 30.0726\n",
      "INFO:tensorflow:step = 501, loss = 0.592412\n",
      "INFO:tensorflow:global_step/sec: 86.1835\n",
      "INFO:tensorflow:step = 601, loss = 0.553362\n",
      "INFO:tensorflow:global_step/sec: 69.1732\n",
      "INFO:tensorflow:step = 701, loss = 0.536757\n",
      "INFO:tensorflow:global_step/sec: 74.3031\n",
      "INFO:tensorflow:step = 801, loss = 0.713545\n",
      "INFO:tensorflow:global_step/sec: 69.5533\n",
      "INFO:tensorflow:step = 901, loss = 0.517913\n",
      "INFO:tensorflow:global_step/sec: 73.5552\n",
      "INFO:tensorflow:step = 1001, loss = 0.510943\n",
      "INFO:tensorflow:global_step/sec: 63.9391\n",
      "INFO:tensorflow:step = 1101, loss = 0.666777\n",
      "INFO:tensorflow:global_step/sec: 83.1485\n",
      "INFO:tensorflow:step = 1201, loss = 0.502534\n",
      "INFO:tensorflow:global_step/sec: 83.8764\n",
      "INFO:tensorflow:step = 1301, loss = 0.497785\n",
      "INFO:tensorflow:global_step/sec: 85.8724\n",
      "INFO:tensorflow:step = 1401, loss = 0.482655\n",
      "INFO:tensorflow:global_step/sec: 80.2362\n",
      "INFO:tensorflow:step = 1501, loss = 0.47422\n",
      "INFO:tensorflow:global_step/sec: 71.2463\n",
      "INFO:tensorflow:step = 1601, loss = 0.517687\n",
      "INFO:tensorflow:global_step/sec: 85.303\n",
      "INFO:tensorflow:step = 1701, loss = 0.484636\n",
      "INFO:tensorflow:global_step/sec: 80.586\n",
      "INFO:tensorflow:step = 1801, loss = 0.476911\n",
      "INFO:tensorflow:global_step/sec: 68.3769\n",
      "INFO:tensorflow:step = 1901, loss = 0.462461\n",
      "INFO:tensorflow:global_step/sec: 76.9283\n",
      "INFO:tensorflow:step = 2001, loss = 0.479174\n",
      "INFO:tensorflow:Saving checkpoints for 2076 into ./dnn/kaggle/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 55.3701\n",
      "INFO:tensorflow:step = 2101, loss = 0.455312\n",
      "INFO:tensorflow:global_step/sec: 78.9561\n",
      "INFO:tensorflow:step = 2201, loss = 0.443116\n",
      "INFO:tensorflow:global_step/sec: 76.3748\n",
      "INFO:tensorflow:step = 2301, loss = 0.450031\n",
      "INFO:tensorflow:global_step/sec: 85.2989\n",
      "INFO:tensorflow:step = 2401, loss = 0.438212\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:657: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:657: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-10-01:51:09\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-10-01:51:10\n",
      "INFO:tensorflow:Saving dict for global step 2076: accuracy = 0.679907, auc = 0.876559, global_step = 2076, loss = 2.66122\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 2500): accuracy = 0.679907, global_step = 2076, auc = 0.876559, loss = 2.66122\n",
      "INFO:tensorflow:Stopping. Best step: 500 with loss = 1.4135351181030273.\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into ./dnn/kaggle/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.43095.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'embedding_lr_multipliers': None, 'optimizer': <tensorflow.python.training.adam.AdamOptimizer object at 0x116e8bfd0>, 'dropout': None, 'activation_fn': <function relu at 0x113078158>, 'gradient_clip_norm': None, 'input_layer_min_slice_size': None, 'hidden_units': [100, 50, 25], 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x116e8bd30>, 'feature_columns': (_RealValuedColumn(column_name='', dimension=17, default_value=None, dtype=tf.float32, normalizer=None),)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as learn\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "# Encode feature vector\n",
    "encode_numeric_zscore(df_train,'len')\n",
    "encode_numeric_zscore(df_train,'links')\n",
    "df_train.drop('title', axis=1, inplace=True)\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "num_classes = len(df_train.groupby('class')['class'].nunique())\n",
    "\n",
    "print(\"Number of classes: {}\".format(num_classes))\n",
    "\n",
    "# Create x & y for training\n",
    "\n",
    "# Create the x-side (feature vectors) of the training\n",
    "x, y = to_xy(df_train,'class')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Get/clear a directory to store the neural network to\n",
    "model_dir = get_model_dir('kaggle',True)\n",
    "\n",
    "opt=tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "#opt=tf.train.MomentumOptimizer(learning_rate=1e-5,momentum=0.9)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 30, 20, 5\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[1])]\n",
    "classifier = learn.DNNClassifier(\n",
    "    optimizer= opt,\n",
    "    model_dir= model_dir,\n",
    "    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=30),\n",
    "    hidden_units=[100, 50, 25], n_classes=num_classes, feature_columns=feature_columns)\n",
    "\n",
    "# Might be needed in future versions of \"TensorFlow Learn\"\n",
    "#classifier = learn.SKCompat(classifier) # For Sklearn compatibility\n",
    "\n",
    "# Early stopping\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_validate,\n",
    "    y_validate,\n",
    "    every_n_steps=500,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=50)\n",
    "\n",
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following code builds the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:409: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id       class-0       class-1       class-2       class-3  \\\n",
      "0       6639  9.999897e-01  9.921896e-07  1.065219e-23  9.280530e-06   \n",
      "1       9603  0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00   \n",
      "2      12234  9.825366e-01  1.737450e-02  1.105266e-08  8.891991e-05   \n",
      "3      16535  2.608014e-35  1.000000e+00  1.050461e-36  0.000000e+00   \n",
      "4      18157  1.000000e+00  1.667878e-13  0.000000e+00  0.000000e+00   \n",
      "5      33302  0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00   \n",
      "6      37190  0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00   \n",
      "7      43051  2.216956e-07  9.999998e-01  8.821426e-25  0.000000e+00   \n",
      "8      43373  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "9      51487  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "10     54573  0.000000e+00  1.000000e+00  5.470123e-18  0.000000e+00   \n",
      "11     87367  3.828980e-10  7.078284e-22  1.000000e+00  2.773656e-19   \n",
      "12    134848  4.482552e-01  4.734279e-05  5.627338e-25  5.516837e-01   \n",
      "13    141876  9.999815e-01  1.144969e-10  3.680989e-37  1.842468e-05   \n",
      "14    145313  1.000000e+00  8.645820e-11  5.336010e-12  2.564547e-19   \n",
      "15    145540  9.999987e-01  4.315527e-08  5.447342e-08  1.303514e-06   \n",
      "16    153664  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "17    153669  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "18    153696  6.453224e-02  1.206060e-01  8.148617e-01  6.719075e-16   \n",
      "19    155030  9.854350e-01  1.229691e-02  4.424307e-14  2.268069e-03   \n",
      "20    157992  4.403835e-14  4.149804e-38  1.000000e+00  0.000000e+00   \n",
      "21    159271  2.071317e-02  9.792868e-01  2.347628e-15  2.499523e-15   \n",
      "22    162411  1.000000e+00  0.000000e+00  0.000000e+00  5.516612e-37   \n",
      "23    171275  1.000000e+00  4.487367e-08  0.000000e+00  0.000000e+00   \n",
      "24    171385  9.308729e-01  1.175571e-06  7.233990e-21  5.352704e-20   \n",
      "25    171757  1.000000e+00  1.236694e-13  0.000000e+00  2.369987e-15   \n",
      "26    172074  1.000000e+00  7.101076e-24  0.000000e+00  1.492467e-08   \n",
      "27    184771  9.999875e-01  2.176011e-07  3.184696e-11  1.226443e-05   \n",
      "28    187244  1.000000e+00  1.136245e-08  7.093298e-11  4.827352e-09   \n",
      "29    194210  9.999990e-01  9.309670e-07  5.489135e-27  5.132190e-13   \n",
      "..       ...           ...           ...           ...           ...   \n",
      "469  2302779  9.999999e-01  1.047807e-07  3.705769e-22  2.987261e-14   \n",
      "470  2311189  3.224646e-09  2.222853e-08  1.000000e+00  2.478403e-14   \n",
      "471  2321267  1.155500e-04  9.998845e-01  2.480462e-18  3.581089e-17   \n",
      "472  2323516  1.347442e-01  2.632078e-01  3.209589e-09  5.870575e-01   \n",
      "473  2331584  7.516464e-03  9.924591e-01  0.000000e+00  1.730243e-05   \n",
      "474  2332284  9.999443e-01  5.566398e-05  2.738799e-09  2.484572e-12   \n",
      "475  2333594  1.000000e+00  1.015739e-10  0.000000e+00  0.000000e+00   \n",
      "476  2334708  9.999998e-01  1.825184e-07  1.164816e-11  4.887003e-09   \n",
      "477  2340383  7.574700e-01  2.343252e-01  1.322853e-03  3.601064e-03   \n",
      "478  2350163  1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "479  2355050  1.000000e+00  4.399837e-22  0.000000e+00  3.962680e-34   \n",
      "480  2356040  1.000000e+00  8.620076e-09  5.979750e-13  2.162280e-09   \n",
      "481  2360336  9.999963e-01  3.711842e-06  5.261624e-09  1.425673e-17   \n",
      "482  2362845  3.351229e-06  9.999967e-01  9.461558e-33  9.690232e-38   \n",
      "483  2367846  9.997590e-01  3.888375e-08  0.000000e+00  2.409108e-04   \n",
      "484  2374265  9.999995e-01  5.032981e-07  1.286981e-17  5.255675e-12   \n",
      "485  2381863  8.799536e-01  8.604026e-09  1.890067e-33  1.200464e-01   \n",
      "486  2387168  1.154295e-05  9.999884e-01  3.621410e-18  2.645276e-13   \n",
      "487  2397174  1.568648e-05  9.999843e-01  1.992235e-17  1.905310e-12   \n",
      "488  2403976  1.000000e+00  5.701585e-10  7.914501e-12  3.034456e-09   \n",
      "489  2404265  4.556295e-07  9.999995e-01  1.144703e-25  9.664898e-23   \n",
      "490  2404589  2.270289e-04  9.997714e-01  1.752794e-15  5.997414e-08   \n",
      "491  2405447  2.258671e-01  9.495721e-07  7.741319e-01  1.416905e-09   \n",
      "492  2406846  9.999949e-01  1.934887e-10  1.785211e-12  5.166001e-06   \n",
      "493  2412637  3.815572e-03  9.961767e-01  1.261884e-12  5.653097e-08   \n",
      "494  2419317  1.000000e+00  2.274750e-20  4.369434e-20  2.062733e-26   \n",
      "495  2419370  4.068518e-01  5.931481e-01  2.758437e-31  0.000000e+00   \n",
      "496  2421907  1.929027e-05  9.999807e-01  1.057202e-22  1.533994e-23   \n",
      "497  2426754  5.829029e-06  9.999942e-01  3.997299e-21  1.733279e-18   \n",
      "498  2427736  1.000000e+00  1.515415e-26  2.747090e-11  0.000000e+00   \n",
      "\n",
      "          class-4  \n",
      "0    6.899193e-13  \n",
      "1    0.000000e+00  \n",
      "2    1.461641e-17  \n",
      "3    0.000000e+00  \n",
      "4    0.000000e+00  \n",
      "5    0.000000e+00  \n",
      "6    0.000000e+00  \n",
      "7    0.000000e+00  \n",
      "8    0.000000e+00  \n",
      "9    0.000000e+00  \n",
      "10   0.000000e+00  \n",
      "11   0.000000e+00  \n",
      "12   1.373424e-05  \n",
      "13   5.674808e-19  \n",
      "14   1.253546e-35  \n",
      "15   5.047617e-26  \n",
      "16   0.000000e+00  \n",
      "17   0.000000e+00  \n",
      "18   2.600444e-23  \n",
      "19   6.289078e-12  \n",
      "20   0.000000e+00  \n",
      "21   6.789545e-13  \n",
      "22   2.673308e-18  \n",
      "23   0.000000e+00  \n",
      "24   6.912598e-02  \n",
      "25   2.371842e-11  \n",
      "26   1.165796e-30  \n",
      "27   2.259852e-29  \n",
      "28   3.474586e-23  \n",
      "29   2.522653e-22  \n",
      "..            ...  \n",
      "469  2.794212e-14  \n",
      "470  7.097062e-33  \n",
      "471  2.248032e-12  \n",
      "472  1.499063e-02  \n",
      "473  7.076977e-06  \n",
      "474  1.090824e-11  \n",
      "475  0.000000e+00  \n",
      "476  5.574400e-16  \n",
      "477  3.280994e-03  \n",
      "478  0.000000e+00  \n",
      "479  0.000000e+00  \n",
      "480  2.074035e-15  \n",
      "481  2.013890e-18  \n",
      "482  2.527303e-27  \n",
      "483  1.290986e-12  \n",
      "484  8.211864e-13  \n",
      "485  6.642135e-09  \n",
      "486  6.177949e-10  \n",
      "487  2.168962e-09  \n",
      "488  2.256948e-17  \n",
      "489  6.430329e-16  \n",
      "490  1.427349e-06  \n",
      "491  2.522000e-29  \n",
      "492  6.607718e-32  \n",
      "493  7.776865e-06  \n",
      "494  0.000000e+00  \n",
      "495  4.690526e-30  \n",
      "496  1.627721e-18  \n",
      "497  1.191065e-13  \n",
      "498  0.000000e+00  \n",
      "\n",
      "[499 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate Kaggle submit file\n",
    "\n",
    "# Encode feature vector\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "#encode_numeric_zscore(df_test,'petal_w')\n",
    "#encode_numeric_zscore(df_test,'petal_l')\n",
    "#encode_numeric_zscore(df_test,'sepal_w')\n",
    "#encode_numeric_zscore(df_test,'sepal_l')\n",
    "encode_numeric_zscore(df_test,'len')\n",
    "encode_numeric_zscore(df_test,'links')\n",
    "df_test.drop('title', axis=1, inplace=True)\n",
    "ids = df_test['id']\n",
    "df_test.drop('id', axis=1, inplace=True)\n",
    "\n",
    "x = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "# Generate predictions\n",
    "pred = list(classifier.predict_proba(x, as_iterable=True))\n",
    "#pred\n",
    "\n",
    "# Create submission data set\n",
    "\n",
    "df_submit = pd.DataFrame(pred)\n",
    "df_submit.insert(0,'id',ids)\n",
    "df_submit.columns = ['id','class-0','class-1','class-2','class-3','class-4']\n",
    "\n",
    "df_submit.to_csv(filename_submit, index=False)\n",
    "\n",
    "print(df_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
