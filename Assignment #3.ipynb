{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Programming Assignment #3 \n",
    "# Hanyu Feng \n",
    "# Student ID:452106\n",
    "# T81-558: Application of Deep Learning\n",
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as learn\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "#from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import time\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "path = \"./assignment3/data\"\n",
    "\n",
    "# Encode a text field to dummy variables\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode a text field to a single index value\n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Encode a numeric field to Z-Scores\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "    if sd ==0:\n",
    "        df[name] = df[name]\n",
    "    else :\n",
    "        df[name] = (df[name]-mean)/sd\n",
    "    \n",
    "\n",
    "\n",
    "# Encode a numeric field to fill missing values with the median.\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "# Convert a dataframe to x/y suitable for training.\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    return df.as_matrix(result),df[target]\n",
    "\n",
    "\n",
    "\n",
    "def get_model_dir(name,erase):\n",
    "    base_path = os.path.join(\"./assignment3\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "\n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Assignment3***\n"
     ]
    }
   ],
   "source": [
    "print(\"***Assignment3***\")\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# Read the data\n",
    "\n",
    "\n",
    "\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_trainencode = os.path.join(path,\"encodetrain.csv\")\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "\n",
    "tempid = df_train['id']\n",
    "temptitle = df_train['title']\n",
    "templen = df_train['len']\n",
    "df_train.drop('id',axis=1,inplace=True)\n",
    "df_train.drop('title',axis=1,inplace=True)\n",
    "\n",
    "encode_numeric_zscore(df_train,'len')\n",
    "#encode_numeric_zscore(df_train,'cite_arXiv')\n",
    "#encode_numeric_zscore(df_train,'cite_av_media')\n",
    "#encode_numeric_zscore(df_train,'cite_book')\n",
    "#encode_numeric_zscore(df_train,'cite_comic')\n",
    "#encode_numeric_zscore(df_train,'cite_conference')\n",
    "#encode_numeric_zscore(df_train,'cite_encyclopedia')\n",
    "##encode_numeric_zscore(df_train,'cite_episode')\n",
    "#encode_numeric_zscore(df_train,'Cite_govtrack')\n",
    "#encode_numeric_zscore(df_train,'cite_journal')\n",
    "#encode_numeric_zscore(df_train,'cite_magazine')\n",
    "#encode_numeric_zscore(df_train,'cite_press_release')\n",
    "encode_numeric_zscore(df_train,'cite_web')\n",
    "encode_numeric_zscore(df_train,'links')\n",
    "#encode_numeric_zscore(df_train,'tables')\n",
    "encode_numeric_zscore(df_train,'files')\n",
    "#encode_numeric_zscore(df_train,'math')\n",
    "\n",
    "df_trainencode = df_train\n",
    "df_trainencode.to_csv(filename_trainencode, index=False)\n",
    "#df_train.drop('len',axis=1,inplace=True)\n",
    "\n",
    "classnum = encode_text_index(df_train,'class')\n",
    "num_classes = len(classnum)\n",
    "\n",
    "#print(df_train)\n",
    "\n",
    "# Preprocess the data\n",
    "#df.to_csv(filename_write,index=False)\n",
    "\n",
    "#\n",
    "#print(df)\n",
    "\n",
    "# Split the data\n",
    "x,y = to_xy(df_train,'class')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "x, y, test_size=0.25)\n",
    "\n",
    "# Get/clear a directory to store the neural network to\n",
    "model_dir = get_model_dir('Wiki',True)\n",
    "\n",
    "# Choose an optimizer\n",
    "opt=tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "classifier = learn.DNNClassifier(\n",
    "    model_dir= model_dir, \n",
    "    optimizer=opt,\n",
    "    dropout = 0.01,\n",
    "    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1),\n",
    "    hidden_units=[8,16,4], n_classes=num_classes, feature_columns=feature_columns)\n",
    "\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=50,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=500)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train,monitors=[validation_monitor],  steps=5000)# monitors=[validation_monitor]\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n",
    "\n",
    "print(\"Best step: {}, Last successful step: {}\".format(\n",
    "validation_monitor.best_step,validation_monitor._last_successful_step))\n",
    "\n",
    "# Don't display numpy in scientific notation\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "pred = list(classifier.predict(x_test, as_iterable=True))\n",
    "score1 = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy score: {}\".format(score1))\n",
    "\n",
    "pred = list(classifier.predict_proba(x_test, as_iterable=True))\n",
    "score2 = metrics.log_loss(y_test,pred)\n",
    "print(\"Log loss score: {}\".format(score2))\n",
    "score = score1/score2\n",
    "print(\"Total score: {}\".format('%.3f'%score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id   class-0       class-1       class-2       class-3       class-4\n",
      "0       6639  0.630670  2.183953e-02  2.722955e-03  3.334488e-01  1.131922e-02\n",
      "1       9603  0.052670  6.129435e-01  3.343655e-01  2.111747e-05  3.926943e-11\n",
      "2      12234  0.746944  1.472519e-01  2.143760e-02  8.406988e-02  2.963127e-04\n",
      "3      16535  0.013201  4.400193e-01  5.467795e-01  4.489084e-07  3.859145e-14\n",
      "4      18157  0.982462  3.048287e-04  9.600405e-04  1.622228e-02  5.082044e-05\n",
      "5      33302  0.007809  3.742804e-01  6.179103e-01  1.133390e-07  3.372657e-15\n",
      "6      37190  0.149001  6.746165e-01  1.758357e-01  5.464346e-04  1.601994e-08\n",
      "7      43051  0.208046  6.599697e-01  1.302272e-01  1.756926e-03  1.456208e-07\n",
      "8      43373  0.002893  2.660013e-01  7.311057e-01  9.113484e-09  4.042695e-17\n",
      "9      51487  0.342525  5.755535e-01  6.908134e-02  1.283311e-02  6.807683e-06\n",
      "10     54573  0.175047  6.706213e-01  1.533815e-01  9.497156e-04  4.535687e-08\n",
      "11     87367  0.378322  4.578809e-01  1.619506e-01  1.846684e-03  1.124100e-07\n",
      "12    134848  0.831459  7.068917e-03  2.306113e-03  1.565524e-01  2.613381e-03\n",
      "13    141876  0.777401  1.047212e-02  2.492696e-03  2.053098e-01  4.324803e-03\n",
      "14    145313  0.910175  3.528025e-02  1.373705e-02  4.072248e-02  8.550520e-05\n",
      "15    145540  0.896401  4.872904e-02  1.754882e-02  3.725643e-02  6.483768e-05\n",
      "16    153664  0.387900  5.340438e-01  5.484687e-02  2.318739e-02  2.201490e-05\n",
      "17    153669  0.355300  5.646406e-01  6.487864e-02  1.517097e-02  9.471621e-06\n",
      "18    153696  0.371559  5.499157e-01  5.976475e-02  1.874619e-02  1.440842e-05\n",
      "19    155030  0.801637  1.046597e-01  1.918828e-02  7.427485e-02  2.398255e-04\n",
      "20    157992  0.229006  6.506938e-01  1.177963e-01  2.503767e-03  2.862879e-07\n",
      "21    159271  0.775049  1.062990e-02  2.499279e-03  2.074131e-01  4.408999e-03\n",
      "22    162411  0.958312  1.171540e-02  2.780241e-02  2.170481e-03  2.364748e-07\n",
      "23    171275  0.948270  1.345317e-03  5.035504e-02  2.982318e-05  5.464457e-11\n",
      "24    171385  0.887606  5.871093e-02  4.790869e-02  5.772993e-03  1.199889e-06\n",
      "25    171757  0.919848  3.688884e-02  1.789127e-02  2.534143e-02  3.069425e-05\n",
      "26    172074  0.987781  1.096094e-03  3.175444e-03  7.940446e-03  7.497804e-06\n",
      "27    184771  0.990526  1.097416e-03  6.719858e-03  1.656781e-03  2.728914e-07\n",
      "28    187244  0.924309  3.316858e-02  1.655007e-02  2.593956e-02  3.320571e-05\n",
      "29    194210  0.696720  1.633747e-02  2.660376e-03  2.765722e-01  7.709529e-03\n",
      "..       ...       ...           ...           ...           ...           ...\n",
      "469  2302779  0.816017  7.996701e-03  2.366907e-03  1.705604e-01  3.059165e-03\n",
      "470  2311189  0.358694  5.616472e-01  6.379063e-02  1.585753e-02  1.033838e-05\n",
      "471  2321267  0.803062  8.802677e-03  2.413032e-03  1.822626e-01  3.459480e-03\n",
      "472  2323516  0.994190  4.212171e-04  2.985739e-03  2.402718e-03  8.049524e-07\n",
      "473  2331584  0.839989  7.138289e-02  1.560038e-02  7.277840e-02  2.494225e-04\n",
      "474  2332284  0.483920  3.854091e-01  2.579458e-02  1.043856e-01  4.903721e-04\n",
      "475  2333594  0.688778  1.696527e-02  2.671119e-03  2.834845e-01  8.101047e-03\n",
      "476  2334708  0.997347  7.913778e-05  1.365923e-03  1.207309e-03  3.163943e-07\n",
      "477  2340383  0.682523  1.901691e-01  2.118611e-02  1.056450e-01  4.763806e-04\n",
      "478  2350163  1.000000  0.000000e+00  1.281748e-26  0.000000e+00  0.000000e+00\n",
      "479  2355050  0.980384  4.358285e-03  8.603302e-03  6.651500e-03  3.370002e-06\n",
      "480  2356040  0.833085  6.973336e-03  2.299316e-03  1.550735e-01  2.568408e-03\n",
      "481  2360336  0.307078  6.031452e-01  8.179436e-02  7.979428e-03  2.683081e-06\n",
      "482  2362845  0.769115  1.103154e-02  2.515384e-03  2.127129e-01  4.625138e-03\n",
      "483  2367846  0.810462  8.339240e-03  2.387204e-03  1.755838e-01  3.227856e-03\n",
      "484  2374265  0.805818  8.629121e-03  2.403572e-03  1.797768e-01  3.372286e-03\n",
      "485  2381863  0.780524  1.026378e-02  2.483773e-03  2.025140e-01  4.214259e-03\n",
      "486  2387168  0.994401  3.906551e-04  2.889356e-03  2.317855e-03  7.637139e-07\n",
      "487  2397174  0.993420  5.337598e-04  3.250753e-03  2.794070e-03  1.028823e-06\n",
      "488  2403976  0.990283  1.219114e-03  5.167485e-03  3.329167e-03  1.153650e-06\n",
      "489  2404265  0.997645  6.546167e-05  1.374262e-03  9.149821e-04  1.868570e-07\n",
      "490  2404589  0.998417  2.639586e-05  8.497572e-04  7.068261e-04  1.437888e-07\n",
      "491  2405447  0.516982  3.291023e-02  2.704178e-03  4.276563e-01  1.974745e-02\n",
      "492  2406846  0.922967  3.399663e-02  1.669028e-02  2.631207e-02  3.399792e-05\n",
      "493  2412637  0.880414  1.186911e-02  4.169188e-03  1.026770e-01  8.708852e-04\n",
      "494  2419317  0.987339  3.735495e-04  1.331650e-03  1.093471e-02  2.064490e-05\n",
      "495  2419370  0.758879  1.173628e-02  2.541522e-03  2.218324e-01  5.010581e-03\n",
      "496  2421907  0.779007  1.036479e-02  2.488133e-03  2.038722e-01  4.267766e-03\n",
      "497  2426754  0.934532  6.464850e-03  3.939342e-03  5.479734e-02  2.664253e-04\n",
      "498  2427736  0.999946  1.483051e-08  4.250793e-05  1.099964e-05  2.258364e-10\n",
      "\n",
      "[499 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#from sklearn import metrics\n",
    "#filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_submit = os.path.join(path,str('%.3f'%score)+\"submit.csv\")\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "tempid = df_test['id']\n",
    "temptitle = df_test['title']\n",
    "\n",
    "\n",
    "encode_numeric_zscore(df_test,'len')\n",
    "#encode_numeric_zscore(df_test,'cite_arXiv')\n",
    "#encode_numeric_zscore(df_test,'cite_av_media')\n",
    "#encode_numeric_zscore(df_test,'cite_book')\n",
    "#encode_numeric_zscore(df_test,'cite_comic')\n",
    "#encode_numeric_zscore(df_test,'cite_conference')\n",
    "#encode_numeric_zscore(df_test,'cite_encyclopedia')\n",
    "#encode_numeric_zscore(df_test,'cite_episode')\n",
    "#encode_numeric_zscore(df_test,'Cite_govtrack')\n",
    "#encode_numeric_zscore(df_test,'cite_journal')\n",
    "#encode_numeric_zscore(df_test,'cite_magazine')\n",
    "#encode_numeric_zscore(df_test,'cite_press_release')\n",
    "encode_numeric_zscore(df_test,'cite_web')\n",
    "encode_numeric_zscore(df_test,'links')\n",
    "#encode_numeric_zscore(df_test,'tables')\n",
    "encode_numeric_zscore(df_test,'files')\n",
    "#encode_numeric_zscore(df_test,'math')\n",
    "\n",
    "#print(df_test)\n",
    "\n",
    "df_test.drop('id',axis=1,inplace=True)\n",
    "df_test.drop('title',axis=1,inplace=True)\n",
    "#df_test.drop('len',axis=1,inplace=True)\n",
    "\n",
    "x = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "# Generate predictions\n",
    "pred = list(classifier.predict_proba(x, as_iterable=True))\n",
    "\n",
    "df_submit = pd.DataFrame(pred)\n",
    "df_submit.insert(0,'id',tempid)\n",
    "df_submit.columns = ['id','class-0','class-1','class-2','class-3','class-4']\n",
    "#df_submit.insert(6,'class-5',0)\n",
    "df_submit.to_csv(filename_submit, index=False)\n",
    "print(df_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nencode_numeric_zscore(df_test,'len')\\n#encode_numeric_zscore(df_test,'cite_arXiv')\\nencode_numeric_zscore(df_test,'cite_av_media')\\nencode_numeric_zscore(df_test,'cite_book')\\n#encode_numeric_zscore(df_test,'cite_comic')\\nencode_numeric_zscore(df_test,'cite_conference')\\nencode_numeric_zscore(df_test,'cite_encyclopedia')\\nencode_numeric_zscore(df_test,'cite_episode')\\n#encode_numeric_zscore(df_test,'Cite_govtrack')\\nencode_numeric_zscore(df_test,'cite_journal')\\nencode_numeric_zscore(df_test,'cite_magazine')\\nencode_numeric_zscore(df_test,'cite_press_release')\\nencode_numeric_zscore(df_test,'cite_web')\\nencode_numeric_zscore(df_test,'links')\\nencode_numeric_zscore(df_test,'tables')\\nencode_numeric_zscore(df_test,'files')\\nencode_numeric_zscore(df_test,'math')\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "encode_numeric_zscore(df_train,'len')\n",
    "#encode_numeric_zscore(df_train,'cite_arXiv')\n",
    "encode_numeric_zscore(df_train,'cite_av_media')\n",
    "encode_numeric_zscore(df_train,'cite_book')\n",
    "#encode_numeric_zscore(df_train,'cite_comic')\n",
    "encode_numeric_zscore(df_train,'cite_conference')\n",
    "encode_numeric_zscore(df_train,'cite_encyclopedia')\n",
    "encode_numeric_zscore(df_train,'cite_episode')\n",
    "#encode_numeric_zscore(df_train,'Cite_govtrack')\n",
    "encode_numeric_zscore(df_train,'cite_journal')\n",
    "encode_numeric_zscore(df_train,'cite_magazine')\n",
    "encode_numeric_zscore(df_train,'cite_press_release')\n",
    "encode_numeric_zscore(df_train,'cite_web')\n",
    "encode_numeric_zscore(df_train,'links')\n",
    "encode_numeric_zscore(df_train,'tables')\n",
    "encode_numeric_zscore(df_train,'files')\n",
    "encode_numeric_zscore(df_train,'math')\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "encode_numeric_zscore(df_test,'len')\n",
    "#encode_numeric_zscore(df_test,'cite_arXiv')\n",
    "encode_numeric_zscore(df_test,'cite_av_media')\n",
    "encode_numeric_zscore(df_test,'cite_book')\n",
    "#encode_numeric_zscore(df_test,'cite_comic')\n",
    "encode_numeric_zscore(df_test,'cite_conference')\n",
    "encode_numeric_zscore(df_test,'cite_encyclopedia')\n",
    "encode_numeric_zscore(df_test,'cite_episode')\n",
    "#encode_numeric_zscore(df_test,'Cite_govtrack')\n",
    "encode_numeric_zscore(df_test,'cite_journal')\n",
    "encode_numeric_zscore(df_test,'cite_magazine')\n",
    "encode_numeric_zscore(df_test,'cite_press_release')\n",
    "encode_numeric_zscore(df_test,'cite_web')\n",
    "encode_numeric_zscore(df_test,'links')\n",
    "encode_numeric_zscore(df_test,'tables')\n",
    "encode_numeric_zscore(df_test,'files')\n",
    "encode_numeric_zscore(df_test,'math')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}